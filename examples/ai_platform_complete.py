#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Complete AI Platform Integration Example

Demonstrates how to integrate Hailo8 TPU support into a complete AI platform
including frontend, backend, ML pipeline, model management and more
"""

import os
import sys
from pathlib import Path

# æ·»åŠ  hailo8_installer åˆ° Python è·¯å¾„
current_dir = Path(__file__).parent
project_root = current_dir.parent
sys.path.insert(0, str(project_root))

from hailo8_installer.integration import integrate_with_existing_project

def create_complete_ai_platform():
    """åˆ›å»ºå®Œæ•´çš„ AI å¹³å°"""
    project_path = "/tmp/complete_ai_platform"
    os.makedirs(project_path, exist_ok=True)
    
    print("ğŸš€ Creating complete AI platform project...")
    
    # Create project structure
    create_project_structure(project_path)
    
    # Create components
    create_backend_service(project_path)
    create_frontend_app(project_path)
    create_ml_pipeline(project_path)
    create_model_registry(project_path)
    create_monitoring_system(project_path)
    create_deployment_configs(project_path)
    create_documentation(project_path)
    
    # Integrate Hailo8 support
    print("ğŸ”§ Integrating Hailo8 TPU support...")
    integrate_with_existing_project(
        project_path=project_path,
        project_name="Complete AI Platform",
        integration_type="detailed",
        config={
            "hailo8_features": [
                "inference_acceleration",
                "model_optimization",
                "performance_monitoring",
                "auto_scaling"
            ],
            "target_platforms": ["linux", "docker"],
            "api_integration": True,
            "monitoring": True,
            "auto_setup": True
        }
    )
    
    print("âœ… Complete AI Platform created successfully!")
    print(f"ğŸ“ Project path: {project_path}")
    print(f"ğŸŒ Start command: cd {project_path} && docker-compose up")
    
    return project_path

def create_project_structure(project_path):
    """åˆ›å»ºé¡¹ç›®ç›®å½•ç»“æ„"""
    directories = [
        "frontend/src/components",
        "frontend/src/pages",
        "frontend/src/utils",
        "frontend/public",
        "backend/src/api",
        "backend/src/models",
        "backend/src/services",
        "backend/src/utils",
        "ml_pipeline/src",
        "ml_pipeline/models",
        "ml_pipeline/data",
        "model_registry/src",
        "model_registry/storage",
        "monitoring/src",
        "monitoring/dashboards",
        "deployment/docker",
        "deployment/kubernetes",
        "docs",
        "scripts",
        "tests/unit",
        "tests/integration",
        "config"
    ]
    
    for directory in directories:
        os.makedirs(f"{project_path}/{directory}", exist_ok=True)

def create_backend_service(project_path):
    """åˆ›å»ºåç«¯æœåŠ¡"""
    # ä¸»åº”ç”¨æ–‡ä»¶
    with open(f"{project_path}/backend/src/main.py", "w") as f:
        f.write("""#!/usr/bin/env python3
from fastapi import FastAPI, HTTPException, Depends, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
from typing import Dict, Any, List, Optional
import logging
import asyncio
import uvicorn

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="Complete AI Platform API",
    description="åŸºäº Hailo8 TPU çš„å®Œæ•´ AI å¹³å°åç«¯æœåŠ¡",
    version="1.0.0"
)

# CORS ä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æ•°æ®æ¨¡å‹
class InferenceRequest(BaseModel):
    model_id: str
    input_data: Any
    parameters: Optional[Dict[str, Any]] = {}

class ModelUpload(BaseModel):
    name: str
    description: str
    model_type: str
    framework: str

# å…¨å±€çŠ¶æ€
system_status = {
    "hailo8_available": True,
    "models_loaded": 3,
    "active_pipelines": 2,
    "total_inferences": 1250
}

@app.get("/")
async def root():
    return {
        "service": "Complete AI Platform API",
        "version": "1.0.0",
        "status": "running",
        "hailo8_enabled": True
    }

@app.get("/api/v1/system/status")
async def get_system_status():
    return system_status

@app.get("/api/v1/models")
async def list_models():
    return [
        {
            "id": "resnet50_hailo",
            "name": "ResNet-50 (Hailo Optimized)",
            "type": "classification",
            "framework": "pytorch",
            "device": "hailo8",
            "performance": {"fps": 120, "latency": "8.3ms"}
        },
        {
            "id": "yolov5_hailo",
            "name": "YOLOv5 (Hailo Optimized)",
            "type": "detection",
            "framework": "pytorch", 
            "device": "hailo8",
            "performance": {"fps": 60, "latency": "16.7ms"}
        }
    ]

@app.post("/api/v1/inference")
async def run_inference(request: InferenceRequest):
    # æ¨¡æ‹Ÿæ¨ç†
    await asyncio.sleep(0.01)  # Hailo8 å¿«é€Ÿæ¨ç†
    
    return {
        "task_id": "task_001",
        "model_id": request.model_id,
        "device": "Hailo8 TPU",
        "inference_time": "8.3ms",
        "result": {
            "predictions": [
                {"class": "cat", "confidence": 0.95},
                {"class": "dog", "confidence": 0.03}
            ]
        }
    }

@app.get("/api/v1/pipelines")
async def list_pipelines():
    return [
        {
            "id": "image_classification_pipeline",
            "name": "å›¾åƒåˆ†ç±»æµæ°´çº¿",
            "status": "running",
            "progress": 0.75
        },
        {
            "id": "object_detection_pipeline", 
            "name": "ç›®æ ‡æ£€æµ‹æµæ°´çº¿",
            "status": "completed",
            "progress": 1.0
        }
    ]

@app.get("/health")
async def health_check():
    return {"status": "healthy", "hailo8": "available"}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
""")
    
    # requirements.txt
    with open(f"{project_path}/backend/requirements.txt", "w") as f:
        f.write("""fastapi>=0.104.0
uvicorn[standard]>=0.24.0
pydantic>=2.0.0
python-multipart>=0.0.6
aiofiles>=23.0.0
numpy>=1.24.0
pillow>=10.0.0
""")

def create_frontend_app(project_path):
    """åˆ›å»ºå‰ç«¯åº”ç”¨"""
    # React ä¸»åº”ç”¨
    with open(f"{project_path}/frontend/src/App.js", "w") as f:
        f.write("""import React, { useState, useEffect } from 'react';
import './App.css';

function App() {
  const [systemStatus, setSystemStatus] = useState({});
  const [models, setModels] = useState([]);
  const [pipelines, setPipelines] = useState([]);

  useEffect(() => {
    fetchSystemStatus();
    fetchModels();
    fetchPipelines();
  }, []);

  const fetchSystemStatus = async () => {
    try {
      const response = await fetch('/api/v1/system/status');
      const data = await response.json();
      setSystemStatus(data);
    } catch (error) {
      console.error('è·å–ç³»ç»ŸçŠ¶æ€å¤±è´¥:', error);
    }
  };

  const fetchModels = async () => {
    try {
      const response = await fetch('/api/v1/models');
      const data = await response.json();
      setModels(data);
    } catch (error) {
      console.error('è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥:', error);
    }
  };

  const fetchPipelines = async () => {
    try {
      const response = await fetch('/api/v1/pipelines');
      const data = await response.json();
      setPipelines(data);
    } catch (error) {
      console.error('è·å–æµæ°´çº¿åˆ—è¡¨å¤±è´¥:', error);
    }
  };

  return (
    <div className="app">
      <header className="app-header">
        <h1>ğŸ§  Complete AI Platform</h1>
        <div className="status-indicator">
          Hailo8 TPU: {systemStatus.hailo8_available ? 'ğŸŸ¢ Online' : 'ğŸ”´ Offline'}
        </div>
      </header>

      <main className="app-main">
        <div className="dashboard">
          <div className="stats-grid">
            <div className="stat-card">
              <h3>ğŸ¤– å·²åŠ è½½æ¨¡å‹</h3>
              <div className="stat-value">{systemStatus.models_loaded || 0}</div>
            </div>
            <div className="stat-card">
              <h3>âš¡ æ´»è·ƒæµæ°´çº¿</h3>
              <div className="stat-value">{systemStatus.active_pipelines || 0}</div>
            </div>
            <div className="stat-card">
              <h3>ğŸ“Š æ€»æ¨ç†æ¬¡æ•°</h3>
              <div className="stat-value">{systemStatus.total_inferences || 0}</div>
            </div>
          </div>

          <div className="content-grid">
            <div className="models-section">
              <h2>ğŸ¤– æ¨¡å‹ç®¡ç†</h2>
              <div className="models-list">
                {models.map(model => (
                  <div key={model.id} className="model-card">
                    <h4>{model.name}</h4>
                    <p>ç±»å‹: {model.type}</p>
                    <p>è®¾å¤‡: {model.device}</p>
                    <p>æ€§èƒ½: {model.performance.fps} FPS</p>
                  </div>
                ))}
              </div>
            </div>

            <div className="pipelines-section">
              <h2>ğŸ”„ ML æµæ°´çº¿</h2>
              <div className="pipelines-list">
                {pipelines.map(pipeline => (
                  <div key={pipeline.id} className="pipeline-card">
                    <h4>{pipeline.name}</h4>
                    <p>çŠ¶æ€: {pipeline.status}</p>
                    <div className="progress-bar">
                      <div 
                        className="progress-fill" 
                        style={{width: `${pipeline.progress * 100}%`}}
                      ></div>
                    </div>
                  </div>
                ))}
              </div>
            </div>
          </div>
        </div>
      </main>
    </div>
  );
}

export default App;
""")
    
    # CSS æ ·å¼
    with open(f"{project_path}/frontend/src/App.css", "w") as f:
        f.write(""".app {
  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
  min-height: 100vh;
  background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
}

.app-header {
  display: flex;
  justify-content: space-between;
  align-items: center;
  padding: 1rem 2rem;
  background: rgba(255, 255, 255, 0.1);
  backdrop-filter: blur(10px);
  color: white;
}

.app-header h1 {
  margin: 0;
  font-size: 1.8rem;
}

.status-indicator {
  padding: 0.5rem 1rem;
  background: rgba(255, 255, 255, 0.2);
  border-radius: 20px;
  font-weight: 500;
}

.app-main {
  padding: 2rem;
}

.dashboard {
  max-width: 1200px;
  margin: 0 auto;
}

.stats-grid {
  display: grid;
  grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
  gap: 1.5rem;
  margin-bottom: 2rem;
}

.stat-card {
  background: rgba(255, 255, 255, 0.9);
  padding: 1.5rem;
  border-radius: 12px;
  text-align: center;
  box-shadow: 0 4px 15px rgba(0, 0, 0, 0.1);
}

.stat-card h3 {
  margin: 0 0 1rem 0;
  color: #666;
  font-size: 0.9rem;
}

.stat-value {
  font-size: 2.5rem;
  font-weight: 700;
  color: #333;
}

.content-grid {
  display: grid;
  grid-template-columns: 1fr 1fr;
  gap: 2rem;
}

.models-section, .pipelines-section {
  background: rgba(255, 255, 255, 0.9);
  padding: 1.5rem;
  border-radius: 12px;
  box-shadow: 0 4px 15px rgba(0, 0, 0, 0.1);
}

.models-section h2, .pipelines-section h2 {
  margin: 0 0 1rem 0;
  color: #333;
}

.models-list, .pipelines-list {
  display: flex;
  flex-direction: column;
  gap: 1rem;
}

.model-card, .pipeline-card {
  padding: 1rem;
  background: #f8f9fa;
  border-radius: 8px;
  border: 1px solid #e9ecef;
}

.model-card h4, .pipeline-card h4 {
  margin: 0 0 0.5rem 0;
  color: #333;
}

.model-card p, .pipeline-card p {
  margin: 0.25rem 0;
  color: #666;
  font-size: 0.9rem;
}

.progress-bar {
  width: 100%;
  height: 8px;
  background: #e9ecef;
  border-radius: 4px;
  overflow: hidden;
  margin-top: 0.5rem;
}

.progress-fill {
  height: 100%;
  background: linear-gradient(90deg, #4CAF50, #8BC34A);
  transition: width 0.3s ease;
}

@media (max-width: 768px) {
  .content-grid {
    grid-template-columns: 1fr;
  }
  
  .stats-grid {
    grid-template-columns: 1fr;
  }
}
""")
    
    # package.json
    with open(f"{project_path}/frontend/package.json", "w") as f:
        f.write("""{
  "name": "complete-ai-platform-frontend",
  "version": "1.0.0",
  "private": true,
  "dependencies": {
    "react": "^18.2.0",
    "react-dom": "^18.2.0",
    "react-scripts": "5.0.1"
  },
  "scripts": {
    "start": "react-scripts start",
    "build": "react-scripts build",
    "test": "react-scripts test"
  },
  "proxy": "http://localhost:8000"
}
""")

def create_ml_pipeline(project_path):
    """åˆ›å»ºæœºå™¨å­¦ä¹ æµæ°´çº¿"""
    with open(f"{project_path}/ml_pipeline/src/pipeline.py", "w") as f:
        f.write("""#!/usr/bin/env python3
import asyncio
import logging
from typing import Dict, Any, List

logger = logging.getLogger(__name__)

class MLPipeline:
    def __init__(self, pipeline_id: str, config: Dict[str, Any]):
        self.pipeline_id = pipeline_id
        self.config = config
        self.status = "created"
        self.steps = []
    
    async def add_step(self, step_name: str, step_config: Dict[str, Any]):
        step = {
            "name": step_name,
            "config": step_config,
            "status": "pending"
        }
        self.steps.append(step)
        logger.info(f"æ·»åŠ æ­¥éª¤ {step_name} åˆ°æµæ°´çº¿ {self.pipeline_id}")
    
    async def run(self):
        logger.info(f"å¼€å§‹è¿è¡Œæµæ°´çº¿ {self.pipeline_id}")
        self.status = "running"
        
        try:
            for step in self.steps:
                await self._run_step(step)
                if step["status"] == "failed":
                    self.status = "failed"
                    return
            
            self.status = "completed"
            logger.info(f"æµæ°´çº¿ {self.pipeline_id} è¿è¡Œå®Œæˆ")
            
        except Exception as e:
            self.status = "failed"
            logger.error(f"æµæ°´çº¿è¿è¡Œå¤±è´¥: {e}")
    
    async def _run_step(self, step: Dict[str, Any]):
        step_name = step["name"]
        logger.info(f"è¿è¡Œæ­¥éª¤ {step_name}")
        
        step["status"] = "running"
        
        try:
            # æ¨¡æ‹Ÿæ­¥éª¤æ‰§è¡Œ
            await asyncio.sleep(1)
            
            if step_name == "data_preprocessing":
                result = {"processed_samples": 10000}
            elif step_name == "model_training":
                result = {"accuracy": 0.95, "loss": 0.05}
            elif step_name == "hailo_optimization":
                result = {"optimized": True, "speedup": "3.2x"}
            else:
                result = {"completed": True}
            
            step["result"] = result
            step["status"] = "completed"
            logger.info(f"æ­¥éª¤ {step_name} å®Œæˆ")
            
        except Exception as e:
            step["error"] = str(e)
            step["status"] = "failed"
            logger.error(f"æ­¥éª¤ {step_name} å¤±è´¥: {e}")

class PipelineManager:
    def __init__(self):
        self.pipelines = {}
    
    async def create_pipeline(self, pipeline_id: str, config: Dict[str, Any]):
        pipeline = MLPipeline(pipeline_id, config)
        
        # æ·»åŠ é»˜è®¤æ­¥éª¤
        await pipeline.add_step("data_preprocessing", {})
        await pipeline.add_step("model_training", {})
        await pipeline.add_step("hailo_optimization", {})
        await pipeline.add_step("deployment", {})
        
        self.pipelines[pipeline_id] = pipeline
        return pipeline
    
    def get_pipeline(self, pipeline_id: str):
        return self.pipelines.get(pipeline_id)
    
    def list_pipelines(self):
        return list(self.pipelines.values())

# å…¨å±€å®ä¾‹
pipeline_manager = PipelineManager()
""")

def create_model_registry(project_path):
    """åˆ›å»ºæ¨¡å‹æ³¨å†Œä¸­å¿ƒ"""
    with open(f"{project_path}/model_registry/src/registry.py", "w") as f:
        f.write("""#!/usr/bin/env python3
import json
import os
from typing import Dict, Any, List, Optional
from pathlib import Path

class ModelRegistry:
    def __init__(self, storage_path: str = "./storage"):
        self.storage_path = Path(storage_path)
        self.storage_path.mkdir(exist_ok=True)
        self.models = self._load_models()
    
    def _load_models(self) -> Dict[str, Any]:
        models_file = self.storage_path / "models.json"
        if models_file.exists():
            with open(models_file, 'r') as f:
                return json.load(f)
        return {}
    
    def _save_models(self):
        models_file = self.storage_path / "models.json"
        with open(models_file, 'w') as f:
            json.dump(self.models, f, indent=2)
    
    def register_model(self, model_id: str, metadata: Dict[str, Any]):
        self.models[model_id] = {
            **metadata,
            "registered_at": "2024-01-01T00:00:00Z",
            "status": "registered"
        }
        self._save_models()
        return self.models[model_id]
    
    def get_model(self, model_id: str) -> Optional[Dict[str, Any]]:
        return self.models.get(model_id)
    
    def list_models(self) -> List[Dict[str, Any]]:
        return list(self.models.values())
    
    def update_model(self, model_id: str, updates: Dict[str, Any]):
        if model_id in self.models:
            self.models[model_id].update(updates)
            self._save_models()
            return self.models[model_id]
        return None
    
    def delete_model(self, model_id: str) -> bool:
        if model_id in self.models:
            del self.models[model_id]
            self._save_models()
            return True
        return False

# å…¨å±€å®ä¾‹
model_registry = ModelRegistry()
""")

def create_monitoring_system(project_path):
    """åˆ›å»ºç›‘æ§ç³»ç»Ÿ"""
    with open(f"{project_path}/monitoring/src/monitor.py", "w") as f:
        f.write("""#!/usr/bin/env python3
import time
import psutil
import logging
from typing import Dict, Any

logger = logging.getLogger(__name__)

class SystemMonitor:
    def __init__(self):
        self.metrics = {}
        self.alerts = []
    
    def collect_metrics(self) -> Dict[str, Any]:
        metrics = {
            "timestamp": time.time(),
            "cpu_usage": psutil.cpu_percent(),
            "memory_usage": psutil.virtual_memory().percent,
            "disk_usage": psutil.disk_usage('/').percent,
            "hailo8_temperature": 45,  # æ¨¡æ‹Ÿ Hailo8 æ¸©åº¦
            "hailo8_utilization": 75,  # æ¨¡æ‹Ÿ Hailo8 åˆ©ç”¨ç‡
            "inference_count": 1250,
            "avg_inference_time": 8.3
        }
        
        self.metrics = metrics
        self._check_alerts(metrics)
        
        return metrics
    
    def _check_alerts(self, metrics: Dict[str, Any]):
        # æ£€æŸ¥å‘Šè­¦æ¡ä»¶
        if metrics["cpu_usage"] > 80:
            self._add_alert("high_cpu", f"CPU ä½¿ç”¨ç‡è¿‡é«˜: {metrics['cpu_usage']}%")
        
        if metrics["hailo8_temperature"] > 70:
            self._add_alert("high_temp", f"Hailo8 æ¸©åº¦è¿‡é«˜: {metrics['hailo8_temperature']}Â°C")
        
        if metrics["avg_inference_time"] > 50:
            self._add_alert("slow_inference", f"æ¨ç†é€Ÿåº¦è¿‡æ…¢: {metrics['avg_inference_time']}ms")
    
    def _add_alert(self, alert_type: str, message: str):
        alert = {
            "type": alert_type,
            "message": message,
            "timestamp": time.time(),
            "severity": "warning"
        }
        self.alerts.append(alert)
        logger.warning(f"å‘Šè­¦: {message}")
    
    def get_alerts(self):
        return self.alerts[-10:]  # è¿”å›æœ€è¿‘10ä¸ªå‘Šè­¦
    
    def clear_alerts(self):
        self.alerts.clear()

# å…¨å±€å®ä¾‹
system_monitor = SystemMonitor()
""")

def create_deployment_configs(project_path):
    """åˆ›å»ºéƒ¨ç½²é…ç½®"""
    # Docker Compose
    with open(f"{project_path}/docker-compose.yml", "w") as f:
        f.write("""version: '3.8'

services:
  frontend:
    build: ./frontend
    ports:
      - "3000:3000"
    depends_on:
      - backend
    environment:
      - REACT_APP_API_URL=http://localhost:8000

  backend:
    build: ./backend
    ports:
      - "8000:8000"
    volumes:
      - ./hailo8:/app/hailo8
      - ./models:/app/models
    environment:
      - HAILO8_ENABLED=true
      - MODEL_STORAGE_PATH=/app/models
    depends_on:
      - redis
      - postgres

  ml-pipeline:
    build: ./ml_pipeline
    volumes:
      - ./data:/app/data
      - ./models:/app/models
    environment:
      - HAILO8_ENABLED=true

  model-registry:
    build: ./model_registry
    volumes:
      - ./models:/app/storage
    ports:
      - "8001:8000"

  monitoring:
    build: ./monitoring
    ports:
      - "8002:8000"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"

  postgres:
    image: postgres:15-alpine
    environment:
      - POSTGRES_DB=aiplatform
      - POSTGRES_USER=admin
      - POSTGRES_PASSWORD=admin123
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"

volumes:
  postgres_data:
""")
    
    # Kubernetes é…ç½®
    with open(f"{project_path}/deployment/kubernetes/deployment.yaml", "w") as f:
        f.write("""apiVersion: apps/v1
kind: Deployment
metadata:
  name: ai-platform-backend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: ai-platform-backend
  template:
    metadata:
      labels:
        app: ai-platform-backend
    spec:
      containers:
      - name: backend
        image: ai-platform-backend:latest
        ports:
        - containerPort: 8000
        env:
        - name: HAILO8_ENABLED
          value: "true"
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "1Gi"
            cpu: "1000m"
---
apiVersion: v1
kind: Service
metadata:
  name: ai-platform-backend-service
spec:
  selector:
    app: ai-platform-backend
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8000
  type: LoadBalancer
""")

def create_documentation(project_path):
    """åˆ›å»ºé¡¹ç›®æ–‡æ¡£"""
    with open(f"{project_path}/README.md", "w") as f:
        f.write("""# Complete AI Platform with Hailo8 TPU

ğŸ§  åŸºäº Hailo8 TPU çš„å®Œæ•´ AI å¹³å°ï¼Œæä¾›ç«¯åˆ°ç«¯çš„æœºå™¨å­¦ä¹ è§£å†³æ–¹æ¡ˆã€‚

## âœ¨ ç‰¹æ€§

- ğŸš€ **Hailo8 TPU åŠ é€Ÿ**: é«˜æ€§èƒ½æ¨ç†åŠ é€Ÿ
- ğŸ¯ **å®Œæ•´ ML æµæ°´çº¿**: ä»æ•°æ®é¢„å¤„ç†åˆ°æ¨¡å‹éƒ¨ç½²
- ğŸ“Š **å®æ—¶ç›‘æ§**: ç³»ç»Ÿæ€§èƒ½å’Œæ¨¡å‹æŒ‡æ ‡ç›‘æ§
- ğŸ”„ **æ¨¡å‹ç®¡ç†**: å®Œæ•´çš„æ¨¡å‹ç”Ÿå‘½å‘¨æœŸç®¡ç†
- ğŸŒ **ç°ä»£åŒ– UI**: React å‰ç«¯ç•Œé¢
- ğŸ³ **å®¹å™¨åŒ–éƒ¨ç½²**: Docker å’Œ Kubernetes æ”¯æŒ

## ğŸ—ï¸ æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend      â”‚    â”‚   Backend API   â”‚    â”‚  ML Pipeline    â”‚
â”‚   (React)       â”‚â—„â”€â”€â–ºâ”‚   (FastAPI)     â”‚â—„â”€â”€â–ºâ”‚   (Python)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Model Registry â”‚    â”‚   Monitoring    â”‚    â”‚   Hailo8 TPU    â”‚
â”‚   (Storage)     â”‚    â”‚   (Metrics)     â”‚    â”‚  (Inference)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ä½¿ç”¨ Docker Compose (æ¨è)

```bash
# å…‹éš†é¡¹ç›®
git clone <repository-url>
cd complete_ai_platform

# å¯åŠ¨æ‰€æœ‰æœåŠ¡
docker-compose up -d

# æŸ¥çœ‹æœåŠ¡çŠ¶æ€
docker-compose ps
```

### æœ¬åœ°å¼€å‘

```bash
# åç«¯æœåŠ¡
cd backend
pip install -r requirements.txt
python src/main.py

# å‰ç«¯æœåŠ¡
cd frontend
npm install
npm start

# ML æµæ°´çº¿
cd ml_pipeline
pip install -r requirements.txt
python src/pipeline.py
```

## ğŸ“‹ æœåŠ¡ç«¯å£

- **å‰ç«¯**: http://localhost:3000
- **åç«¯ API**: http://localhost:8000
- **æ¨¡å‹æ³¨å†Œä¸­å¿ƒ**: http://localhost:8001
- **ç›‘æ§ç³»ç»Ÿ**: http://localhost:8002
- **API æ–‡æ¡£**: http://localhost:8000/docs

## ğŸ”§ é…ç½®

### Hailo8 TPU é…ç½®

```yaml
hailo8:
  enabled: true
  device_id: 0
  optimization_level: 3
  batch_size: 1
```

### æ¨¡å‹é…ç½®

```yaml
models:
  - name: "ResNet-50"
    path: "./models/resnet50_hailo.hef"
    type: "classification"
    input_shape: [1, 224, 224, 3]
  
  - name: "YOLOv5"
    path: "./models/yolov5_hailo.hef"
    type: "detection"
    input_shape: [1, 640, 640, 3]
```

## ğŸ“Š ç›‘æ§æŒ‡æ ‡

- **ç³»ç»ŸæŒ‡æ ‡**: CPUã€å†…å­˜ã€ç£ç›˜ä½¿ç”¨ç‡
- **Hailo8 æŒ‡æ ‡**: æ¸©åº¦ã€åˆ©ç”¨ç‡ã€åŠŸè€—
- **æ¨¡å‹æŒ‡æ ‡**: æ¨ç†å»¶è¿Ÿã€ååé‡ã€å‡†ç¡®ç‡
- **ä¸šåŠ¡æŒ‡æ ‡**: è¯·æ±‚æ•°ã€é”™è¯¯ç‡ã€ç”¨æˆ·æ´»è·ƒåº¦

## ğŸ”„ ML æµæ°´çº¿

1. **æ•°æ®é¢„å¤„ç†**: æ•°æ®æ¸…æ´—ã€å¢å¼ºã€æ ¼å¼è½¬æ¢
2. **æ¨¡å‹è®­ç»ƒ**: æ”¯æŒ PyTorchã€TensorFlow
3. **æ¨¡å‹è¯„ä¼°**: å‡†ç¡®ç‡ã€æ€§èƒ½æŒ‡æ ‡è¯„ä¼°
4. **Hailo8 ä¼˜åŒ–**: æ¨¡å‹é‡åŒ–ã€ç¼–è¯‘ä¼˜åŒ–
5. **æ¨¡å‹éƒ¨ç½²**: è‡ªåŠ¨åŒ–éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ

## ğŸ› ï¸ API æ¥å£

### æ¨ç†æ¥å£

```bash
curl -X POST "http://localhost:8000/api/v1/inference" \\
  -H "Content-Type: application/json" \\
  -d '{
    "model_id": "resnet50_hailo",
    "input_data": "base64_encoded_image"
  }'
```

### æ¨¡å‹ç®¡ç†

```bash
# è·å–æ¨¡å‹åˆ—è¡¨
curl "http://localhost:8000/api/v1/models"

# ä¸Šä¼ æ–°æ¨¡å‹
curl -X POST "http://localhost:8001/api/v1/models" \\
  -F "model=@model.hef" \\
  -F "metadata=@metadata.json"
```

## ğŸ§ª æµ‹è¯•

```bash
# è¿è¡Œå•å…ƒæµ‹è¯•
python -m pytest tests/unit/

# è¿è¡Œé›†æˆæµ‹è¯•
python -m pytest tests/integration/

# æ€§èƒ½æµ‹è¯•
python scripts/benchmark.py
```

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–

### Hailo8 TPU ä¼˜åŒ–

- **æ¨¡å‹é‡åŒ–**: INT8 é‡åŒ–å‡å°‘æ¨¡å‹å¤§å°
- **æ‰¹å¤„ç†**: æé«˜ååé‡
- **æµæ°´çº¿å¹¶è¡Œ**: å¤šæ¨¡å‹å¹¶è¡Œæ¨ç†
- **å†…å­˜ä¼˜åŒ–**: å‡å°‘æ•°æ®ä¼ è¾“å¼€é”€

### ç³»ç»Ÿä¼˜åŒ–

- **ç¼“å­˜ç­–ç•¥**: Redis ç¼“å­˜çƒ­ç‚¹æ•°æ®
- **è´Ÿè½½å‡è¡¡**: å¤šå®ä¾‹éƒ¨ç½²
- **å¼‚æ­¥å¤„ç†**: éé˜»å¡ I/O æ“ä½œ
- **èµ„æºæ± **: è¿æ¥æ± ã€çº¿ç¨‹æ± ç®¡ç†

## ğŸ”’ å®‰å…¨é…ç½®

- **API è®¤è¯**: JWT Token è®¤è¯
- **HTTPS**: SSL/TLS åŠ å¯†ä¼ è¾“
- **è®¿é—®æ§åˆ¶**: åŸºäºè§’è‰²çš„æƒé™ç®¡ç†
- **æ•°æ®åŠ å¯†**: æ•æ„Ÿæ•°æ®åŠ å¯†å­˜å‚¨

## ğŸš€ éƒ¨ç½²æŒ‡å—

### Docker éƒ¨ç½²

```bash
# æ„å»ºé•œåƒ
docker-compose build

# å¯åŠ¨æœåŠ¡
docker-compose up -d

# æ‰©å±•æœåŠ¡
docker-compose up -d --scale backend=3
```

### Kubernetes éƒ¨ç½²

```bash
# åº”ç”¨é…ç½®
kubectl apply -f deployment/kubernetes/

# æŸ¥çœ‹çŠ¶æ€
kubectl get pods
kubectl get services
```

## ğŸ“ å¼€å‘æŒ‡å—

### æ·»åŠ æ–°æ¨¡å‹

1. å‡†å¤‡æ¨¡å‹æ–‡ä»¶ (.hef)
2. åˆ›å»ºæ¨¡å‹é…ç½®æ–‡ä»¶
3. æ³¨å†Œåˆ°æ¨¡å‹æ³¨å†Œä¸­å¿ƒ
4. æ›´æ–° API æ¥å£
5. æ·»åŠ å‰ç«¯ç•Œé¢

### æ‰©å±•åŠŸèƒ½

1. åœ¨å¯¹åº”æœåŠ¡ä¸­æ·»åŠ æ–°åŠŸèƒ½
2. æ›´æ–° API æ–‡æ¡£
3. æ·»åŠ æµ‹è¯•ç”¨ä¾‹
4. æ›´æ–°å‰ç«¯ç•Œé¢

## ğŸ¤ è´¡çŒ®æŒ‡å—

1. Fork é¡¹ç›®
2. åˆ›å»ºåŠŸèƒ½åˆ†æ”¯
3. æäº¤æ›´æ”¹
4. åˆ›å»º Pull Request

## ğŸ“„ è®¸å¯è¯

MIT License

## ğŸ†˜ æ”¯æŒ

- ğŸ“§ é‚®ç®±: support@aiplatform.com
- ğŸ’¬ è®¨è®º: GitHub Discussions
- ğŸ› é—®é¢˜: GitHub Issues
""")

if __name__ == "__main__":
    create_complete_ai_platform()