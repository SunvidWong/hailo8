"""
æ¨ç†æœåŠ¡æ¨¡å—
æä¾›å›¾åƒ/è§†é¢‘æ¨ç†åŠŸèƒ½
"""

import asyncio
import logging
import tempfile
from pathlib import Path
from typing import AsyncGenerator, List, Optional, Union

import grpc
import numpy as np
from fastapi import APIRouter, File, HTTPException, UploadFile, BackgroundTasks
from fastapi.responses import StreamingResponse

from .config import settings
from .models import InferenceRequest, InferenceResponse, StreamInferenceRequest

logger = logging.getLogger(__name__)

# å°è¯•å¯¼å…¥HailoRTï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨æ¨¡æ‹Ÿå®ç°
try:
    import hailo_platform as hpf
    HAILO_AVAILABLE = True
    logger.info("âœ… HailoRT Python API å¯ç”¨")
except ImportError:
    HAILO_AVAILABLE = False
    logger.warning("âš ï¸ HailoRT Python API ä¸å¯ç”¨ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")


class HailoInferenceService:
    """Hailoæ¨ç†æœåŠ¡ç±»"""

    def __init__(self):
        self.device = None
        self.network_group = None
        self.infer_model = None
        self._ready = False
        self._current_model = None

    async def initialize(self):
        """åˆå§‹åŒ–Hailoè®¾å¤‡å’Œæ¨ç†æ¨¡å‹"""
        try:
            if HAILO_AVAILABLE:
                # åˆå§‹åŒ–Hailoè®¾å¤‡
                self.device = hpf.Device()

                # é…ç½®ç½‘ç»œ (éœ€è¦æ ¹æ®å®é™…æ¨¡å‹è°ƒæ•´)
                # è¿™é‡Œåº”è¯¥åŠ è½½é¢„ç¼–è¯‘çš„HEFæ¨¡å‹æ–‡ä»¶
                # model_path = Path(settings.HAILO_MODEL_PATH) / "model.hef"
                # if model_path.exists():
                #     self.network_group = self.device.create_network_group(
                #         hpf.HEF(model_path)
                #     )
                #     self.infer_model = self.network_group.create_infer_model()
                #     self._current_model = str(model_path)
                #     logger.info(f"âœ… åŠ è½½æ¨¡å‹: {model_path}")
                # else:
                #     logger.warning(f"âš ï¸ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")

                logger.info("âœ… Hailoè®¾å¤‡åˆå§‹åŒ–å®Œæˆ")

            self._ready = True
            logger.info("âœ… æ¨ç†æœåŠ¡åˆå§‹åŒ–å®Œæˆ")

        except Exception as e:
            logger.error(f"âŒ æ¨ç†æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
            raise

    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            if self.infer_model:
                self.infer_model.release()
            if self.network_group:
                self.network_group.release()
            if self.device:
                self.device.release()

            logger.info("âœ… æ¨ç†æœåŠ¡èµ„æºæ¸…ç†å®Œæˆ")
        except Exception as e:
            logger.error(f"âŒ èµ„æºæ¸…ç†å¤±è´¥: {e}")

    def is_ready(self) -> bool:
        """æ£€æŸ¥æœåŠ¡æ˜¯å¦å°±ç»ª"""
        return self._ready

    async def get_device_status(self) -> dict:
        """è·å–è®¾å¤‡çŠ¶æ€"""
        if not HAILO_AVAILABLE:
            return {
                "status": "simulation_mode",
                "device_id": "simulated",
                "temperature": 0.0,
                "power_usage": 0.0,
                "memory_usage": 0.0
            }

        try:
            # è·å–å®é™…è®¾å¤‡çŠ¶æ€
            return {
                "status": "ready" if self._ready else "initializing",
                "device_id": getattr(self.device, 'device_id', 'unknown'),
                "temperature": 0.0,  # éœ€è¦æ ¹æ®å®é™…APIè·å–
                "power_usage": 0.0,  # éœ€è¦æ ¹æ®å®é™…APIè·å–
                "memory_usage": 0.0   # éœ€è¦æ ¹æ®å®é™…APIè·å–
            }
        except Exception as e:
            logger.error(f"è·å–è®¾å¤‡çŠ¶æ€å¤±è´¥: {e}")
            return {"status": "error", "error": str(e)}

    async def run_inference(self, request: InferenceRequest) -> InferenceResponse:
        """æ‰§è¡Œæ¨ç†"""
        if not self._ready:
            raise HTTPException(status_code=503, detail="æ¨ç†æœåŠ¡æœªå°±ç»ª")

        try:
            # æ¨¡æ‹Ÿæ¨ç†è¿‡ç¨‹
            await asyncio.sleep(0.1)  # æ¨¡æ‹Ÿæ¨ç†å»¶è¿Ÿ

            if HAILO_AVAILABLE and self.infer_model:
                # å®é™…çš„Hailoæ¨ç†é€»è¾‘
                # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„è¾“å…¥æ•°æ®æ ¼å¼å’Œæ¨¡å‹è¦æ±‚æ¥å®ç°
                pass
            else:
                # æ¨¡æ‹Ÿæ¨ç†ç»“æœ
                results = {
                    "predictions": [
                        {"class": "example", "confidence": 0.95, "bbox": [0, 0, 100, 100]}
                    ],
                    "inference_time": 0.1,
                    "model": self._current_model or "simulated"
                }

            return InferenceResponse(
                success=True,
                results=results,
                processing_time=0.1,
                model_name=self._current_model or "simulated"
            )

        except Exception as e:
            logger.error(f"æ¨ç†å¤±è´¥: {e}")
            raise HTTPException(status_code=500, detail=f"æ¨ç†å¤±è´¥: {str(e)}")

    async def stream_inference(
        self,
        request: StreamInferenceRequest
    ) -> AsyncGenerator[InferenceResponse, None]:
        """æµå¼æ¨ç†"""
        if not self._ready:
            raise HTTPException(status_code=503, detail="æ¨ç†æœåŠ¡æœªå°±ç»ª")

        try:
            # æ¨¡æ‹Ÿæµå¼æ¨ç†
            for i in range(request.max_frames or 10):
                result = await self.run_inference(
                    InferenceRequest(
                        model_name=request.model_name,
                        input_data=f"frame_{i}"
                    )
                )
                yield result
                await asyncio.sleep(0.1)  # æ¨¡æ‹Ÿå¸§é—´éš”

        except Exception as e:
            logger.error(f"æµå¼æ¨ç†å¤±è´¥: {e}")
            raise


# å…¨å±€æ¨ç†æœåŠ¡å®ä¾‹
inference_service = HailoInferenceService()

# åˆ›å»ºè·¯ç”±å™¨
router = APIRouter()


@router.post("/single", response_model=InferenceResponse)
async def single_inference(request: InferenceRequest):
    """å•æ¬¡æ¨ç†æ¥å£"""
    return await inference_service.run_inference(request)


@router.post("/image")
async def inference_from_image(
    file: UploadFile = File(...),
    model_name: Optional[str] = None,
    confidence_threshold: float = 0.5
):
    """ä»å›¾åƒæ–‡ä»¶è¿›è¡Œæ¨ç†"""

    # éªŒè¯æ–‡ä»¶ç±»å‹
    if not file.content_type.startswith('image/'):
        raise HTTPException(status_code=400, detail="åªæ”¯æŒå›¾åƒæ–‡ä»¶")

    try:
        # è¯»å–å›¾åƒæ•°æ®
        image_data = await file.read()

        # ä¿å­˜ä¸´æ—¶æ–‡ä»¶ (å¯é€‰)
        with tempfile.NamedTemporaryFile(delete=False, suffix='.jpg') as temp_file:
            temp_file.write(image_data)
            temp_file.flush()

            # è½¬æ¢ä¸ºnumpyæ•°ç»„ (éœ€è¦å®é™…çš„å›¾åƒå¤„ç†é€»è¾‘)
            # image = cv2.imread(temp_file.name)
            # image_array = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        # æ‰§è¡Œæ¨ç†
        request = InferenceRequest(
            model_name=model_name,
            input_data=image_data,
            confidence_threshold=confidence_threshold
        )

        result = await inference_service.run_inference(request)

        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        Path(temp_file.name).unlink(missing_ok=True)

        return result

    except Exception as e:
        logger.error(f"å›¾åƒæ¨ç†å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"å›¾åƒæ¨ç†å¤±è´¥: {str(e)}")


@router.post("/stream")
async def stream_inference_endpoint(request: StreamInferenceRequest):
    """æµå¼æ¨ç†æ¥å£"""

    async def generate_stream():
        async for result in inference_service.stream_inference(request):
            yield f"data: {result.json()}\n\n"

    return StreamingResponse(
        generate_stream(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
        }
    )


@router.get("/status")
async def get_inference_status():
    """è·å–æ¨ç†æœåŠ¡çŠ¶æ€"""
    return {
        "ready": inference_service.is_ready(),
        "device_status": await inference_service.get_device_status(),
        "current_model": inference_service._current_model,
        "hailo_available": HAILO_AVAILABLE
    }


# gRPCæœåŠ¡å‡½æ•°
async def start_grpc_server(port: int):
    """å¯åŠ¨gRPCæœåŠ¡å™¨"""
    try:
        # è¿™é‡Œéœ€è¦å®ç°å®é™…çš„gRPCæœåŠ¡å™¨
        # åˆ›å»ºgRPCæœåŠ¡å™¨
        server = grpc.aio.server()

        # æ·»åŠ æ¨ç†æœåŠ¡ (éœ€è¦å®ç°protoæ–‡ä»¶å’Œserviceç±»)
        # inference_pb2_grpc.add_InferenceServicer_to_server(
        #     InferenceServiceServicer(), server
        # )

        # ç»‘å®šç«¯å£
        listen_addr = f'[::]:{port}'
        server.add_insecure_port(listen_addr)

        # å¯åŠ¨æœåŠ¡å™¨
        await server.start()
        logger.info(f"ğŸš€ gRPCæœåŠ¡å™¨å¯åŠ¨åœ¨ç«¯å£ {port}")

        # ç­‰å¾…æœåŠ¡å™¨å…³é—­
        await server.wait_for_termination()

    except Exception as e:
        logger.error(f"âŒ gRPCæœåŠ¡å™¨å¯åŠ¨å¤±è´¥: {e}")
        raise


# gRPCæœåŠ¡å®ç°ç±» (éœ€è¦æ ¹æ®protoæ–‡ä»¶å®ç°)
class InferenceServiceServicer:
    """gRPCæ¨ç†æœåŠ¡å®ç°"""

    async def Inference(self, request, context):
        """gRPCæ¨ç†æ–¹æ³•"""
        try:
            # è½¬æ¢gRPCè¯·æ±‚åˆ°å†…éƒ¨æ ¼å¼
            internal_request = InferenceRequest(
                model_name=request.model_name,
                input_data=request.input_data,
                confidence_threshold=request.confidence_threshold
            )

            # æ‰§è¡Œæ¨ç†
            result = await inference_service.run_inference(internal_request)

            # è½¬æ¢ä¸ºgRPCå“åº”æ ¼å¼
            # return inference_pb2.InferenceResponse(
            #     success=result.success,
            #     results=result.results,
            #     processing_time=result.processing_time,
            #     model_name=result.model_name
            # )

        except Exception as e:
            logger.error(f"gRPCæ¨ç†å¤±è´¥: {e}")
            # è¿”å›é”™è¯¯å“åº”
            context.set_code(grpc.StatusCode.INTERNAL)
            context.set_details(str(e))